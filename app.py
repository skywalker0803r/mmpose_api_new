import cv2
import numpy as np
import os
import sys
import tempfile
import json_tricks as json

# å°å…¥ FastAPI å’Œç›¸é—œå·¥å…·
from fastapi import FastAPI, File, UploadFile, BackgroundTasks
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware

# å°å…¥ mmpose å’Œ mmdet çš„æ ¸å¿ƒå‡½å¼
from mmpose.apis import init_model as init_pose_estimator, inference_topdown
from mmdet.apis import init_detector, inference_detector
from mmpose.registry import VISUALIZERS
from mmpose.structures import merge_data_samples, split_instances
from mmpose.utils import adapt_mmdet_pipeline
from mmpose.evaluation.functional import nms

print("--- æ­£åœ¨å•Ÿå‹• FastAPI æ‡‰ç”¨ (å«è¿½è¹¤é‚è¼¯) ---")

# --- 1. æ¨¡å‹åˆå§‹åŒ– (ä¿æŒä¸è®Š) ---
det_config = 'mmdetection/configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py'
det_checkpoint = 'checkpoints/mmdet/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
pose_config = 'mmpose/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py'
pose_checkpoint = 'checkpoints/mmpose/td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'

try:
    print("[åˆå§‹åŒ–] æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
    detector = init_detector(det_config, det_checkpoint, device='cuda:0')
    detector.cfg = adapt_mmdet_pipeline(detector.cfg)
    pose_estimator = init_pose_estimator(pose_config, pose_checkpoint, device='cuda:0')
    print("âœ… æ‰€æœ‰æ¨¡å‹å’Œå·¥å…·åˆå§‹åŒ–æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–éç¨‹ä¸­ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
    sys.exit()

# --- 2. å»ºç«‹ FastAPI æ‡‰ç”¨ (ä¿æŒä¸è®Š) ---
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- è¼”åŠ©å‡½å¼ ---
def clean_numpy(obj):
    if isinstance(obj, np.ndarray): return obj.tolist()
    if isinstance(obj, (np.float32, np.float64)): return float(obj)
    if isinstance(obj, (np.int32, np.int64)): return int(obj)
    if isinstance(obj, dict): return {k: clean_numpy(v) for k, v in obj.items()}
    if isinstance(obj, list): return [clean_numpy(i) for i in obj]
    return obj

def iou(box1, box2):
    """è¨ˆç®—å…©å€‹ bounding box çš„ IoU (äº¤ä¸¦æ¯”)"""
    x1, y1, x2, y2 = box1[0], box1[1], box1[2], box1[3]
    x3, y3, x4, y4 = box2[0], box2[1], box2[2], box2[3]
    inter_x1, inter_y1 = max(x1, x3), max(y1, y3)
    inter_x2, inter_y2 = min(x2, x4), min(y2, y4)
    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)
    area1, area2 = (x2 - x1) * (y2 - y1), (x4 - x3) * (y4 - y3)
    union_area = area1 + area2 - inter_area
    return inter_area / union_area if union_area > 0 else 0

# --- 3. API è·¯ç”± (Endpoint) ---
@app.post("/pose_video")
async def estimate_pose_video(file: UploadFile = File(...), background_tasks: BackgroundTasks = BackgroundTasks()):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video_file:
        video_path = temp_video_file.name
        temp_video_file.write(await file.read())
    
    background_tasks.add_task(os.remove, video_path)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return JSONResponse(status_code=500, content={"error": "ç„¡æ³•é–‹å•Ÿå½±ç‰‡æª”æ¡ˆ"})

    frame_idx = 0
    frame_results = []
    
    # ğŸ’¡ã€è¿½è¹¤é‚è¼¯ã€‘åˆå§‹åŒ– tracked_bboxï¼Œç”¨ä¾†è¨˜ä½ä¸Šä¸€å¹€çš„æŠ•æ‰‹ä½ç½®
    tracked_bbox = None

    try:
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            
            print(f"âœ… æ­£åœ¨è™•ç†ç¬¬ {frame_idx} å¹€çš„éª¨æ¶æ•¸æ“š...")

            det_result = inference_detector(detector, frame)
            pred_instance = det_result.pred_instances.cpu().numpy()
            person_bboxes = pred_instance.bboxes[pred_instance.labels == 0]
            person_scores = pred_instance.scores[pred_instance.labels == 0]

            current_bbox = None
            if len(person_bboxes) > 0:
                # ğŸ’¡ã€è¿½è¹¤é‚è¼¯ã€‘
                if tracked_bbox is None:
                    # å¦‚æœæ˜¯ç¬¬ä¸€å¹€æˆ–ä¹‹å‰è·Ÿä¸Ÿäº†ï¼Œå°±æ‰¾åˆ†æ•¸æœ€é«˜çš„äºº
                    best_bbox_idx = np.argmax(person_scores)
                    current_bbox = person_bboxes[best_bbox_idx]
                    tracked_bbox = current_bbox # è¨˜ä½é€™å€‹äººçš„ä½ç½®
                else:
                    # å¦‚æœæœ‰è¿½è¹¤ç›®æ¨™ï¼Œå°±è¨ˆç®— IoU æ‰¾å‡ºæœ€åƒçš„äºº
                    ious = [iou(tracked_bbox, box) for box in person_bboxes]
                    best_bbox_idx = np.argmax(ious)
                    # å¦‚æœæœ€åƒçš„äºº IoU > 0.3ï¼Œå°±èªå®šæ˜¯ä»–
                    if ious[best_bbox_idx] > 0.3:
                        current_bbox = person_bboxes[best_bbox_idx]
                        tracked_bbox = current_bbox # æ›´æ–°è¿½è¹¤ç›®æ¨™çš„ä½ç½®
                    else:
                        # å¦‚æœ IoU å¤ªä½ï¼Œä»£è¡¨å¯èƒ½è·Ÿä¸Ÿäº†ï¼Œä¸‹ä¸€å¹€é‡æ–°æ‰¾åˆ†æ•¸æœ€é«˜çš„
                        tracked_bbox = None 
            
            # å¦‚æœé€™ä¸€å¹€æœ€çµ‚æ²’æœ‰æ‰¾åˆ°ç›®æ¨™ï¼Œå°±è¨˜éŒ„ç©ºçµæœ
            if current_bbox is None:
                frame_results.append({"frame_idx": frame_idx, "predictions": []})
                frame_idx += 1
                continue

            # å°æ‰¾åˆ°çš„ç›®æ¨™é€²è¡Œå§¿å‹¢ä¼°è¨ˆ
            bboxes_for_pose = [current_bbox]
            pose_results = inference_topdown(pose_estimator, frame, bboxes_for_pose)
            data_samples = merge_data_samples(pose_results)

            # è½‰æ›ç‚º JSON
            predictions = data_samples.get("pred_instances", None)
            split_preds = split_instances(predictions)
            cleaned_preds = clean_numpy(split_preds)

            frame_results.append({
                "frame_idx": frame_idx,
                "predictions": cleaned_preds,
            })
            
            frame_idx += 1
    finally:
        cap.release()
    
    return JSONResponse(content={"frames": frame_results})